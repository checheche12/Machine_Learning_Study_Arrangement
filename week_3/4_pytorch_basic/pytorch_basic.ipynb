{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329eb81f",
   "metadata": {},
   "source": [
    "우선 그 다음으로 넘어가기 전에, pytorch 기본 문법을 잠깐 보고 갈 것이다.\n",
    "\n",
    "직접 아무것도 안 보고 짤 정도는 아니더라도, 적어도 읽고 얘가 뭔 말을 하는건지는 이해할 수 있어야 하니까, 간단하게 설명을 좀 하고 넘어가겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cde60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# gpu 를 사용 가능한지, 아닌지는 아래와 같이 확인 가능.\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# torch 는 모든 데이터를 tensor 로 처리한다.\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c3d525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m y = torch.tensor([\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m])\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 아래와 같이 y를 만들면, 초기화할 때 바로 gpu 에 만들 수 있다. 이렇게 만들면 오류가 발생하지 않는다.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# y = torch.tensor([1.0, 2.0, 3.0], device=\"cuda\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  cpu 에서 처리한 데이터를 gpu 로 명시적으로 옮겨줘야 하기때문에\n",
    "  gpu 를 사용중이라면 to(\"cuda:장비번호\") 와 같은 식으로 데이터를 옮겨주어야 한다.\n",
    "  참고로 같은 장비에 있지 않다면 연산이 불가능하다.\n",
    "'''\n",
    "\n",
    "# print(torch.cuda.is_available()) -> false 였다면 아래의 명령어는 동작하지 않는다.\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda\")\n",
    "print(x)\n",
    "\n",
    "y = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# 아래와 같이 y를 만들면, 초기화할 때 바로 gpu 에 만들 수 있다. 이렇게 만들면 오류가 발생하지 않는다.\n",
    "# y = torch.tensor([1.0, 2.0, 3.0], device=\"cuda\")\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1f85ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.gradient:  None | b.gradient:  None\n",
      "\n",
      "y: 0.0\n",
      "L: 25.0\n",
      "\n",
      "w.gradient:  tensor(-30.) | b.gradient:  tensor(-10.)\n",
      "\n",
      "업데이트 후 다시 계산한 결과:\n",
      "updated w: 0.29999998211860657 | updated b: 0.09999999403953552\n",
      "y2: 1.0\n",
      "L2: 16.0\n"
     ]
    }
   ],
   "source": [
    "# pytorch 는 모델 코드를 작성하면, 계산 그래프를 자동으로 만들고, backward() 를 호출하면\n",
    "# backpropagation 을 자동으로 계산한다.\n",
    "\n",
    "# requires_grad = True 라면 Loss에 연관되어 있는 값 들일때 Loss을 backward 하면\n",
    "# 자동으로 미분 계산에 들어가고,\n",
    "# False 라면 미분 계산에 들어가지 않는다 (Freeze 된다.)\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# 이 시점에선 미분값이 존재하지 않는다.\n",
    "print(\"w.gradient: \", w.grad, \"| b.gradient: \", b.grad)\n",
    "\n",
    "y = w * 3 + b\n",
    "L = (y - 5) ** 2\n",
    "\n",
    "# dy/dw = 3, dy/db = 1\n",
    "# dL/dy = 2(y-5)\n",
    "# 알고싶은게 dL/dw 와 dL/db\n",
    "\n",
    "'''\n",
    "(편의상 d 를 편미분 기호로 사용 함.)\n",
    "dL/dw = dL/dy * dy/dw\n",
    "dL/dy 와 dy/dw 는 이미 알고 있다. 각각 2(y-5) 와 3\n",
    "즉 dL/dw 는 6(y-5)\n",
    "\n",
    "마찬가지로\n",
    "dL/db = dL/dy * dy/db\n",
    "dy/db = 1 이고 dL/dy 는 2(y-5)\n",
    "\n",
    "dL/db = 2(y-5)\n",
    "\n",
    "결과값인 y가 0이 나왔기 때문에 (w=0, b=0 넣고 y = 3w + b 계산하면 y=0 이다.)\n",
    "L은 25 가 나오고\n",
    "각 미분값은 w.grad = -30, b.grad = -10 이 나와야 한다.\n",
    "\n",
    "저 값들을 Learning Rate 와 곱해서 그 값만큼 이동시키면, 조금 더 정답에 가까워 지게 된다.\n",
    "\n",
    "그래서 Loss 를 최소화 시키는 방향으로 Loss 함수를 미분해서 이동시키면 Loss 가 줄어드는 방향으로 이동하는 것이다.\n",
    "\n",
    "그렇기 때문에 Loss 가 학습을 결정하는 가장 중요한 요소!\n",
    "'''\n",
    "\n",
    "# gradient 를 여기서 계산한다.\n",
    "L.backward()\n",
    "\n",
    "print()\n",
    "# 결과값\n",
    "print(\"y:\", y.item())\n",
    "print(\"L:\", L.item())\n",
    "\n",
    "# 이제 미분값이 존재한다.\n",
    "print()\n",
    "print(\"w.gradient: \", w.grad, \"| b.gradient: \", b.grad)\n",
    "\n",
    "\n",
    "# Learning Rate 를 작은 값을 써야, 천천히 이동하면서 안정적으로 수렴 가능하다.\n",
    "# LR 이 크면, 오히려 왔다갔다 하면서 발산 해 버린다.\n",
    "# 해당 예제에서도 lr 을 0.1 을 쓰게 되면, L이 그대로 25 가 나오게 된다. 미분값도 얘네가 미친 영향이 동일해서, 값도 변하지 않는다.\n",
    "# 0.1 보다 크면 발산한다. (Over Shooting)\n",
    "# 0.05 보다 크고, 0.1 보다 작으면 감쇠 진동하면서 수렴한다.(lr=0.05 일때 한번에 정답을 맞춘다. L2 가 0이 되는 값.)\n",
    "'''\n",
    "| 스텝 | (w, b) | y  | Loss |\n",
    "| -- | ------ | -- | ---- |\n",
    "| 0  | (0, 0) | 0  | 25   |\n",
    "| 1  | (3, 1) | 10 | 25   |\n",
    "| 2  | (0, 0) | 0  | 25   |\n",
    "| 3  | (3, 1) | 10 | 25   |\n",
    "... (이하 중략)\n",
    "'''\n",
    "lr = 0.01\n",
    "\n",
    "# 미분 없이 값만 수정한다는 뜻.\n",
    "# loss 에서 나온 값을 미분한 값만큼 \"빼 주어야 한다.\"\n",
    "with torch.no_grad():\n",
    "    w -= lr * w.grad\n",
    "    b -= lr * b.grad\n",
    "\n",
    "# gradient 는 누적되므로, 다시 학습하려면 미분값을 초기화해야 한다\n",
    "w.grad = None\n",
    "b.grad = None\n",
    "\n",
    "# 업데이트 후 forward 계산\n",
    "y2 = w * 3 + b\n",
    "L2 = (y2 - 5) ** 2\n",
    "\n",
    "print(\"\\n업데이트 후 다시 계산한 결과:\")\n",
    "print(\"updated w:\", w.item(), \"| updated b:\", b.item())\n",
    "print(\"y2:\", y2.item())\n",
    "# L2 가 줄었다.\n",
    "print(\"L2:\", L2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 에서 모든 모델은 nn.Module 을 상속 받아서 만든다.\n",
    "# 이렇게 만들어야 모델이 안에 있는 모든 파라미터들을 자동으로 관리 할 수 있다.\n",
    "# 그 외에도 아래 있는 모든 기능을 제공한다.\n",
    "'''\n",
    "PyTorch에서 nn.Module은 다음 기능을 제공함:\n",
    "내부에 포함된 모든 nn.Parameter (즉 requires_grad=True인 tensor)를 자동으로 등록\n",
    "내부에 포함된 모든 서브 모듈(nn.Linear, nn.Conv2d 등)을 자동으로 등록\n",
    ".parameters(), .named_parameters() 로 파라미터 모으는 기능 제공\n",
    ".to(device) 를 하면 내부 파라미터 전부를 GPU/CPU로 자동 이동\n",
    ".eval() / .train() 같은 mode-switch 기능 제공\n",
    ".state_dict() / .load_state_dict() 로 저장/로드 가능\n",
    "'''\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ### Multi Perceptron Layer 에서 입력 차원이 784 차원 / 출력차원이 128 차원 이라는 것이다.\n",
    "        self.fc = nn.Linear(784, 128)\n",
    "\n",
    "    ### forward 는 순방향 전파. 그냥 계산하는 과정이다.\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f64e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch 는 Activation 도 nn.module 형태로 제공한다.\n",
    "'''\n",
    "nn.ReLU()\n",
    "nn.Sigmoid()\n",
    "nn.Tanh()\n",
    "와 같다.\n",
    "'''\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1.0])\n",
    "print(x) # 1\n",
    "x = F.relu(x)\n",
    "print(x) # 1\n",
    "x = torch.tensor([-1.0])\n",
    "x = F.relu(x) # 0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적으로 많이 사용하는 함수는 nn 내에 정의가 되어 있다.\n",
    "# 새로운 Loss 의 경우 종종 직접 구현 해야 할 필요가 있다.\n",
    "\n",
    "# MSE = Mean Square Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 아래와 같이 쓸 수 있다.\n",
    "'''\n",
    "loss = criterion(pred, target)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer는 Gradient 를 보고 weight 를 어떻게 업데이트 할지 정한다.\n",
    "# Adam 을 많이 사용하나, 다른걸 쓸 때도 있으니 그럴땐 공식 document 를 찾아보면 좋다.\n",
    "# 보통 논문에는 어떤 Optimizer 를 썼는지, Optimizer 의 Parameter 까지 같이 제공한다.\n",
    "\n",
    "'''\n",
    "# 아까는 Leraning Rate 를 직접 곱했는데, 실제로는 직접 곱하지 않는다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# gradient 계산\n",
    "loss.backward()    \n",
    "# 가중치 업데이트\n",
    "optimizer.step()   \n",
    "# gradient 초기화, 위에서 하나짜리 업데이트 할 때는 직접 초기화 했지만, 실제로는 이렇게 초기화 한다.\n",
    "optimizer.zero_grad()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110792e",
   "metadata": {},
   "source": [
    "이 외에 필요한 Dataset 읽기 같은 경우는 필요할 때마다 주석으로 언급하도록 하겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
